{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find stellar parameters from measured frequencies\n",
    "\n",
    "This notebook finds stellar parameters from measured frequencies. Provided all dependencies are installed, it should work right out of the box.\n",
    "\n",
    "First import some things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import h5py\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the network architecture and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance(predictions, targets, inputs):\n",
    "    return tf.nn.l2_loss(tf.subtract(predictions, targets), name='l2')\n",
    "\n",
    "net = tflearn.input_data(shape=[None, 8])\n",
    "net = tflearn.batch_normalization(net)\n",
    "net = tflearn.fully_connected(net, 500, activation='relu', regularizer='L2')\n",
    "net = tflearn.fully_connected(net, 500, activation='relu', regularizer='L2')\n",
    "net = tflearn.fully_connected(net, 500, activation='relu', regularizer='L2')\n",
    "net = tflearn.fully_connected(net, 500, activation='relu', regularizer='L2')\n",
    "net = tflearn.fully_connected(net, 500, activation='relu', regularizer='L2')\n",
    "net = tflearn.fully_connected(net, 500, activation='relu', regularizer='L2')\n",
    "net = tflearn.fully_connected(net, 5000, activation='relu', regularizer='L2')\n",
    "net = tflearn.fully_connected(net, 5000, activation='relu', regularizer='L2')\n",
    "net = tflearn.fully_connected(net, 5000, activation='tanh', regularizer='L2')\n",
    "net = tflearn.fully_connected(net, 1, activation='linear', regularizer='L2')\n",
    "\n",
    "net = tflearn.regression(net, metric=None, loss='mean_square')\n",
    "model = tflearn.DNN(net, checkpoint_path='./model_checkpoints')\n",
    "\n",
    "# model.load('./models/run_star_8.tflearn')\n",
    "model.load('./models/dnn.tflearn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the lower and upper bound from the data\n",
    "\n",
    "This  sets the initial random sampling box, these are the limits from the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound_low = [2., 0.012, 0.68, 0.01, 1, 0.01]\n",
    "bound_high = [20., 0.02, 0.72, 0.05, 1000, 0.72]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for the genetic algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up random sampling and particle filter functions\n",
    "def random_sample():\n",
    "    return [[np.random.uniform(bound_low[i], bound_high[i]) for i in range(len(bound_low))] for x in range(settings[\"initial_iteration_size\"])]\n",
    "\n",
    "def gauss_sample(seeds, i, spread):\n",
    "    def bound(low, high, value):\n",
    "        return max(low, min(high, value))\n",
    "    def gauss_sample_single(seed):\n",
    "        return [\n",
    "            bound(bound_low[i], bound_high[i], np.random.normal(seed[i], seed[i]*spread))\n",
    "                for i in range(len(seed) - 1)\n",
    "        ]\n",
    "    return [gauss_sample_single(random.choice(seeds)) for x in range(settings['iteration_size'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the loss function\n",
    "\n",
    "Please don't touch this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_point(point):\n",
    "    inputs = []\n",
    "    for l in range(3):\n",
    "        for n_pg in range(-50, 6):\n",
    "            inputs.append(point + [l] + [n_pg])\n",
    "    outputs = model.predict(inputs)\n",
    "    return outputs\n",
    "\n",
    "def get_point_index(l, n_pg):\n",
    "    return 56*l + n_pg + 50\n",
    "def get_error(y1, y2):\n",
    "    return (y1-y2)**2\n",
    "\n",
    "# Set up function to determine error of a point\n",
    "def point_error(points, verbose = False):\n",
    "    global model\n",
    "    errors = []\n",
    "    \n",
    "    if verbose == True:\n",
    "        best_list = []\n",
    "    \n",
    "    for point in tqdm(points):\n",
    "        # Append l and n_pg to all points\n",
    "        y_pred = calculate_point(point)\n",
    "        y_true = star['data']\n",
    "            \n",
    "        if star['settings']['match_mode'] == 'relative':\n",
    "            min_err = math.inf\n",
    "            # Only with fixed l and relative but fixed n_pg\n",
    "            # Take min and max n_pg values to scan around\n",
    "            min_n_pg = -50\n",
    "            if 'min_n_pg' in star['settings']:\n",
    "                min_n_pg = star['settings']['min_n_pg']\n",
    "            \n",
    "            max_n_pg = 5\n",
    "            if 'max_n_pg' in star['settings']:\n",
    "                max_n_pg = star['settings']['max_n_pg']\n",
    "            max_n_pg = max_n_pg - y_true[-1]['n_pg']\n",
    "            \n",
    "            # Calculate least error\n",
    "            for n_pg_idx in range(min_n_pg, max_n_pg):\n",
    "                current_error = 0\n",
    "                \n",
    "                for y in y_true:\n",
    "                    if type(y['l']).__name__ == 'int':\n",
    "                        point_index = get_point_index(y['l'], n_pg_idx+y['n_pg'])\n",
    "                        current_error += get_error(y_pred[point_index], y['freq'])\n",
    "                    else:\n",
    "                        min_l_err = math.inf\n",
    "                        for l in y['l']:\n",
    "                            point_index = get_point_index(l, n_pg_idx+y['n_pg'])\n",
    "                            current_l_error = get_error(y_pred[point_index], y['freq'])\n",
    "                            if current_l_error < min_l_err:\n",
    "                                min_l_err = current_l_error\n",
    "                        current_error += min_l_err\n",
    "                if current_error < min_err:\n",
    "                    min_err = current_error\n",
    "                    best_n_pg_idx = n_pg_idx\n",
    "                \n",
    "            if verbose == True:\n",
    "                best_list.append(best_n_pg_idx)\n",
    "            point_error = min_err\n",
    "        else:\n",
    "            # Exact match: calculate error for all star samples independently\n",
    "            point_error = 0\n",
    "            max_n_pg = 5\n",
    "            if 'max_n_pg' in star['settings']:\n",
    "                max_n_pg = star['settings']['max_n_pg']\n",
    "            prev_n_pg = [max_n_pg, max_n_pg]\n",
    "            for idx, y in enumerate(y_true):\n",
    "                # Exact match\n",
    "                if 'l' in y and 'n_pg' in y:\n",
    "                    if type(y['l']).__name__ == 'int':\n",
    "                        n_pg = y['n_pg']\n",
    "                        if n_pg == 'prev':\n",
    "                            n_pg = prev_n_pg[y['l'] - 1]\n",
    "                        \n",
    "                        point_index = get_point_index(y['l'], n_pg)\n",
    "                        point_error += get_error(y_pred[point_index], y['freq'])\n",
    "                        prev_l = y['l']\n",
    "                        prev_n_pg = n_pg\n",
    "                        if verbose == True:\n",
    "                            best_list.append([prev_l, prev_n_pg])\n",
    "                    else:\n",
    "                        min_err = math.inf\n",
    "                        for l in y['l']:\n",
    "                            point_index = get_point_index(l, y['n_pg'])\n",
    "                            current_error = get_error(y_pred[point_index], y['freq'])\n",
    "                            if current_error < min_err:\n",
    "                                min_err = current_error\n",
    "                                prev_l = l\n",
    "                                prev_n_pg[l - 1] = y['n_pg']\n",
    "                        point_error += min_err\n",
    "                        if verbose == True:\n",
    "                            best_list.append([prev_l, prev_n_pg])\n",
    "                elif 'l' in y and 'n_pg_max' in y:\n",
    "                    min_n_pg = -50\n",
    "                    if 'min_n_pg' in star['settings']:\n",
    "                        min_n_pg = star['settings']['min_n_pg']\n",
    "\n",
    "                    n_pg_min = min_n_pg + len(y_true)-1 - idx\n",
    "                    if 'n_pn_min' in y:\n",
    "                        n_pg_min = y['n_pg_min']\n",
    "\n",
    "                    if type(y['l']).__name__ == 'int':\n",
    "                        \n",
    "                        # Get the max n_pg\n",
    "                        n_pg_max = y['n_pg_max']\n",
    "                        if (n_pg_max == 'prev'):\n",
    "                            n_pg_max = prev_n_pg[y['l'] - 1]\n",
    "                        \n",
    "                        min_err = math.inf\n",
    "                        for n_pg in range(n_pg_min, n_pg_max):\n",
    "                            point_index = get_point_index(y['l'], n_pg)\n",
    "                            current_error = get_error(y_pred[point_index], y['freq'])\n",
    "                            if current_error < min_err:\n",
    "                                min_err = current_error\n",
    "                                prev_l_x = y['l']\n",
    "                                prev_n_pg_x = n_pg\n",
    "                        prev_l = prev_l_x\n",
    "                        prev_n_pg[prev_l_x - 1] = prev_n_pg_x\n",
    "                        point_error += min_err\n",
    "                        if verbose == True:\n",
    "                            best_list.append([prev_l, prev_n_pg[prev_l - 1]])\n",
    "                    else:\n",
    "                        min_err = math.inf\n",
    "                        for l in y['l']:\n",
    "                            # Get the max n_pg\n",
    "                            n_pg_max = y['n_pg_max']\n",
    "                            if (n_pg_max == 'prev'):\n",
    "                                n_pg_max = prev_n_pg[l - 1]\n",
    "                            for n_pg in range(n_pg_min, n_pg_max):\n",
    "                                point_index = get_point_index(l, n_pg)\n",
    "                                current_error = get_error(y_pred[point_index], y['freq'])\n",
    "                                if current_error < min_err:\n",
    "                                    min_err = current_error\n",
    "                                    prev_l_x = l\n",
    "                                    prev_n_pg_x = n_pg\n",
    "                                    \n",
    "                        prev_l = prev_l_x\n",
    "                        prev_n_pg[prev_l_x - 1] = prev_n_pg_x\n",
    "                        point_error += min_err\n",
    "                        if verbose == True:\n",
    "                            best_list.append([prev_l, prev_n_pg[prev_l - 1]])\n",
    "                else:\n",
    "                    print(\"Could not parse\", y)\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "        point_error /= len(star['data'])\n",
    "        errors.append(point_error)\n",
    "    if verbose == True:\n",
    "        return np.array(errors), best_list\n",
    "    return np.array(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plot(Data, save_to_disk = False):\n",
    "    fig = plt.figure(figsize=(30,20))\n",
    "    plt.rc('text', usetex=True)\n",
    "    def add_plot(fig, fig_key, x, y, x_label, c_label):\n",
    "        ax = fig.add_subplot(3,3,fig_key)\n",
    "        ax.scatter(Data[:,x], Data[:,y])\n",
    "        ax.set_xlabel(x_label, size=25)\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_ylabel(c_label, size=25)\n",
    "        axes = fig.gca()\n",
    "        ax.xaxis.set_tick_params(labelsize=25)\n",
    "        ax.yaxis.set_tick_params(labelsize=25)\n",
    "        axes.set_xlim([np.min(Data[:,x]), np.max(Data[:,x])])\n",
    "        axes.set_ylim([np.min(Data[:,y])/1.2, np.max(Data[:,y])*1.2])\n",
    "\n",
    "    add_plot(fig, 1, 0, 6, r'$M (M_\\odot)$', r'$\\chi^2$')\n",
    "    add_plot(fig, 2, 3, 6, r'$f_{\\rm ov}$', r'$\\chi^2$')\n",
    "    add_plot(fig, 3, 1, 6, 'Z', r'$\\chi^2$')\n",
    "    add_plot(fig, 4, 5, 6, r'$X_c$', r'$\\chi^2$')\n",
    "    add_plot(fig, 5, 4, 6, r'$D_{\\rm mix} (cm^2\\,s^{-1}$', r'$\\chi^2$')\n",
    "    add_plot(fig, 6, 2, 6, r'X', r'$\\chi^2$')\n",
    "\n",
    "    if save_to_disk == True:\n",
    "        fig.savefig('./stars/results/'+star['settings']['filename']+'.pdf', pad_inches=0, bbox_inches='tight')\n",
    "        print(\"Saved to disk\")\n",
    "    else:\n",
    "        clear_output()\n",
    "        fig.show()\n",
    "        plt.pause(0.0001)\n",
    "        \n",
    "def create_plot2(Data, Data2, save_to_disk = False):\n",
    "    fig = plt.figure(figsize=(30,20))\n",
    "    plt.rc('text', usetex=True)\n",
    "    def add_plot(fig, fig_key, x, y, x_label, c_label):\n",
    "        ax = fig.add_subplot(3,3,fig_key)\n",
    "        ax.scatter(Data[:,x], Data[:,y], alpha=0.5, c='#cccccc')\n",
    "        ax.scatter(Data2[:,x], Data2[:,y], alpha=0.5)\n",
    "        ax.set_xlabel(x_label, size=25)\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_ylabel(c_label, size=25)\n",
    "        axes = fig.gca()\n",
    "        ax.xaxis.set_tick_params(labelsize=25)\n",
    "        ax.yaxis.set_tick_params(labelsize=25)\n",
    "        axes.set_xlim([np.min(Data[:,x]), np.max(Data[:,x])])\n",
    "        axes.set_ylim([np.min(Data[:,y])/1.2, np.max(Data[:,y])*1.2])\n",
    "\n",
    "    add_plot(fig, 1, 0, 6, r'$M (M_\\odot)$', r'$\\chi^2$')\n",
    "    add_plot(fig, 2, 3, 6, r'$f_{\\rm ov}$', r'$\\chi^2$')\n",
    "    add_plot(fig, 3, 1, 6, 'Z', r'$\\chi^2$')\n",
    "    add_plot(fig, 4, 5, 6, r'$X_c$', r'$\\chi^2$')\n",
    "    add_plot(fig, 5, 4, 6, r'$D_{\\rm mix} (cm^2\\,s^{-1})$', r'$\\chi^2$')\n",
    "    add_plot(fig, 6, 2, 6, r'X', r'$\\chi^2$')\n",
    "\n",
    "    if save_to_disk == True:\n",
    "        fig.savefig('./stars/results/'+star['settings']['filename']+'.pdf', pad_inches=0, bbox_inches='tight')\n",
    "        fig.savefig('./stars/results/'+star['settings']['filename']+'.png', pad_inches=0, bbox_inches='tight')\n",
    "        print(\"Saved to disk\")\n",
    "    else:\n",
    "        clear_output()\n",
    "        fig.show()\n",
    "        plt.pause(0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run genetic algorithm on specific star\n",
    "The JSON structure should be fairly straightforward. The settings can be left alone or changed if you want to run the genetic algorithm longer or shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for starname in ['star-1', 'star-2', 'star-3', 'star-5', 'star-6', 'star-7']:\n",
    "    star = json.load(open('./stars/'+starname+'.json'))\n",
    "    \n",
    "    settings = {\n",
    "        \"initial_iteration_size\": 5000,\n",
    "        \"iteration_size\": 2000,\n",
    "        \"iteration_count\": 20, # In this setup, the total number of iterations is 3*iteration_count to optimize around the found minima.\n",
    "        \"active_points\": 200\n",
    "    }\n",
    "\n",
    "    iteration_num=1\n",
    "    result_list_big = np.empty((0, 7))\n",
    "    for i in tqdm(range(3*settings['iteration_count'])):\n",
    "        if (i == 0):\n",
    "            sample = random_sample()\n",
    "        elif i < settings['iteration_count']:\n",
    "            sample = gauss_sample(result_list_big[:settings['active_points']], i, 0.4)\n",
    "        elif i < settings['iteration_count']*2:\n",
    "            sample = gauss_sample(result_list_big[:settings['active_points']], i, 0.1)\n",
    "        else:\n",
    "            sample = gauss_sample(result_list_big[:settings['active_points']], i, 0.01)\n",
    "        errors = point_error(sample)\n",
    "        errors = np.array(errors).reshape(len(errors), 1)\n",
    "        result = np.hstack((np.array(sample), errors))\n",
    "        result_list_big = np.vstack((result_list_big, result))\n",
    "        result_list_big = np.array(sorted(result_list_big, key=lambda row: row[-1]))\n",
    "    \n",
    "    settings = {\n",
    "        \"initial_iteration_size\": 1000,\n",
    "        \"iteration_size\": 100,\n",
    "        \"iteration_count\": 10, # In this setup, the total number of iterations is 3*iteration_count to optimize around the found minima.\n",
    "        \"active_points\": 10\n",
    "    }\n",
    "    iteration_num=1\n",
    "    result_list_small = np.empty((0, 7))\n",
    "    for i in tqdm(range(3*settings['iteration_count'])):\n",
    "        if (i == 0):\n",
    "            sample = random_sample()\n",
    "        elif i < settings['iteration_count']:\n",
    "            sample = gauss_sample(result_list_small[:settings['active_points']], i, 0.4)\n",
    "        elif i < settings['iteration_count']*2:\n",
    "            sample = gauss_sample(result_list_small[:settings['active_points']], i, 0.1)\n",
    "        else:\n",
    "            sample = gauss_sample(result_list_small[:settings['active_points']], i, 0.01)\n",
    "        errors = point_error(sample)\n",
    "        errors = np.array(errors).reshape(len(errors), 1)\n",
    "        result = np.hstack((np.array(sample), errors))\n",
    "        result_list_small = np.vstack((result_list_small, result))\n",
    "        result_list_small = np.array(sorted(result_list_small, key=lambda row: row[-1]))\n",
    "\n",
    "    print(\"Done\")\n",
    "    print(result_list_big.shape)\n",
    "    print(result_list_small.shape)\n",
    "    \n",
    "    # Plot and write results file\n",
    "    create_plot2(result_list_big, result_list_small, True)\n",
    "    inputs = [result_list_big.tolist()[0][:-1]]\n",
    "    errors = point_error(inputs, True)\n",
    "\n",
    "    text_file = open(\"./stars/results/\"+star['settings']['filename']+\".txt\", \"w\")\n",
    "    text_file.write(\"Best point big:\\n\")\n",
    "\n",
    "    text_file.write(\"M: %s\\n\" % inputs[0][0])\n",
    "    text_file.write(\"Z: %s\\n\" % inputs[0][1])\n",
    "    text_file.write(\"Xi: %s\\n\" % inputs[0][2])\n",
    "    text_file.write(\"ov: %s\\n\" % inputs[0][3])\n",
    "    text_file.write(\"D: %s\\n\" % inputs[0][4])\n",
    "    text_file.write(\"center_h1: %s\\n\" % inputs[0][5])\n",
    "    text_file.write(\"Best point has error: %s\\n\\n\" % errors[0][0][0])\n",
    "    \n",
    "    inputs = [result_list_small.tolist()[0][:-1]]\n",
    "    errors = point_error(inputs, True)\n",
    "\n",
    "    text_file.write(\"Best point small:\\n\")\n",
    "\n",
    "    text_file.write(\"M: %s\\n\" % inputs[0][0])\n",
    "    text_file.write(\"Z: %s\\n\" % inputs[0][1])\n",
    "    text_file.write(\"Xi: %s\\n\" % inputs[0][2])\n",
    "    text_file.write(\"ov: %s\\n\" % inputs[0][3])\n",
    "    text_file.write(\"D: %s\\n\" % inputs[0][4])\n",
    "    text_file.write(\"center_h1: %s\\n\" % inputs[0][5])\n",
    "    text_file.write(\"Best point has error: %s\" % errors[0][0][0])\n",
    "\n",
    "    if star['settings']['match_mode'] == 'relative':\n",
    "        text_file.write(\"\\nn_pg absolute index: %s\" % errors[1][0])\n",
    "    else:\n",
    "        text_file.write(\"\\nfreq, l, n_pg value table:\\n\")\n",
    "        for i in range(len(star['data'])):\n",
    "            text_file.write(\"%s %s %s\\n\" % (star['data'][i]['freq'], *(errors[1][i])))\n",
    "\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
